{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain_huggingface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:13:37.197000Z","iopub.execute_input":"2025-07-08T17:13:37.197681Z","iopub.status.idle":"2025-07-08T17:13:45.504238Z","shell.execute_reply.started":"2025-07-08T17:13:37.197654Z","shell.execute_reply":"2025-07-08T17:13:45.503512Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain_huggingface\n  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\nCollecting langchain-core<1.0.0,>=0.3.65 (from langchain_huggingface)\n  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.1)\nRequirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.0)\nCollecting langsmith>=0.3.45 (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface)\n  Downloading langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.30.2->langchain_huggingface)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.11.4)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2025.4.26)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.14.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.1)\nDownloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\nDownloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langsmith-0.4.4-py3-none-any.whl (367 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain_huggingface\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.3.23\n    Uninstalling langsmith-0.3.23:\n      Successfully uninstalled langsmith-0.3.23\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.50\n    Uninstalling langchain-core-0.3.50:\n      Successfully uninstalled langchain-core-0.3.50\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nlangchain 0.3.22 requires langsmith<0.4,>=0.1.17, but you have langsmith 0.4.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-core-0.3.68 langchain_huggingface-0.3.0 langsmith-0.4.4 packaging-24.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\nllm = HuggingFacePipeline.from_model_id(\n        model_id = \"microsoft/Phi-3-mini-4k-instruct\",\n        task = \"text-generation\",\n        pipeline_kwargs = dict(\n            temperature = 0.1\n        )\n    )\nmodel = ChatHuggingFace(llm=llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:14:04.007516Z","iopub.execute_input":"2025-07-08T17:14:04.008102Z","iopub.status.idle":"2025-07-08T17:16:01.501698Z","shell.execute_reply.started":"2025-07-08T17:14:04.008072Z","shell.execute_reply":"2025-07-08T17:16:01.500870Z"}},"outputs":[{"name":"stderr","text":"2025-07-08 17:14:26.002479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751994866.367244      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751994866.473744      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"877998f9e1984603bc4e0d22d66d27b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501a3a90064446bc93644e14de43e25a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6873b4cb7d894e7ca5d08bd53dfca2bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d3f04a92434fc1b738cbd18248216b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab0364123dd24878b7a8f6bb618c34ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d7c03033fda413faa20b6a4923457c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea47c55d2f4f41ed9369ffeb8bbe7d53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7643e720d027460c8f3da688ebca85a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e3e72dd266470284f3c2e5f868c839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea67e482dc9542ee8e627e8984990a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"197e08d533654004a7dc3926194f895a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aebf7cd373b414493111c62b2737a25"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Build simple chatbot","metadata":{}},{"cell_type":"code","source":"while True:\n    user_input = input(\"You: \")\n    if user_input == \"exit\":\n        break\n    result = model.invoke(user_input)\n    print(\"AI : \", result.content.split(\"<|assistant|>\\n\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:16:04.816351Z","iopub.execute_input":"2025-07-08T17:16:04.817307Z","iopub.status.idle":"2025-07-08T17:16:42.093462Z","shell.execute_reply.started":"2025-07-08T17:16:04.817280Z","shell.execute_reply":"2025-07-08T17:16:42.092931Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Hi\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI :   Hello! How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Which number is bigger 10 or 12\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI :   The number 12 is bigger than 10.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  multiple bigger number with 10\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI :   To find a multiple of 10 that is bigger than a given number, you simply need to\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"}],"execution_count":3},{"cell_type":"markdown","source":"#### Above example shows that simple chatbot does not have any memory. 12 is bigger number but it didn't retain in next conversation","metadata":{}},{"cell_type":"markdown","source":"## Solution1: Maintain a chat history manually ","metadata":{}},{"cell_type":"code","source":"chat_history = []\nwhile True:\n    user_input = input(\"You: \")\n    chat_history.append(user_input)\n    if user_input == \"exit\":\n        break\n    result = model.invoke(chat_history)\n    ai_reply = result.content.split('<|assistant|>\\n')[1]\n    chat_history.append(ai_reply)\n    print(\"AI: \", ai_reply)\n\nprint(chat_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:16:45.313213Z","iopub.execute_input":"2025-07-08T17:16:45.313796Z","iopub.status.idle":"2025-07-08T17:17:16.620931Z","shell.execute_reply.started":"2025-07-08T17:16:45.313772Z","shell.execute_reply":"2025-07-08T17:17:16.620255Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Hi\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI:   Hello! How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  which number is bigger 12 or 10\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI:   The number 12 is bigger than 10. When comparing two numbers, the larger number\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  add 10 to bigger number\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI:   To add 10 to the bigger number, which is 12, you simply perform the\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"['Hi', ' Hello! How can I assist you today?', 'which number is bigger 12 or 10', ' The number 12 is bigger than 10. When comparing two numbers, the larger number', 'add 10 to bigger number', ' To add 10 to the bigger number, which is 12, you simply perform the', 'exit']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### its caught that bigger number is 12 to perform addition using chat history. \n\n['Hi', ' Hello! How can I assist you today?', 'which number is bigger 12 or 10', ' The number 12 is bigger than 10. When comparing two numbers, the larger number', 'add 10 to bigger number', ' To add 10 to the bigger number, which is 12, you simply perform the', 'exit']\n\nThis is difficult to track who sends which messages - conversation buffer turns huge\nLangChain has already identified this issue","metadata":{}},{"cell_type":"markdown","source":"## System Message, Human Message and AI Message","metadata":{}},{"cell_type":"code","source":"!pip install langchain_core","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:19:54.143522Z","iopub.execute_input":"2025-07-08T17:19:54.144313Z","iopub.status.idle":"2025-07-08T17:19:58.341479Z","shell.execute_reply.started":"2025-07-08T17:19:54.144286Z","shell.execute_reply":"2025-07-08T17:19:58.340495Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.68)\nRequirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (0.4.4)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.4)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (3.10.16)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (2.32.3)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.4.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (2.4.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.3.1)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from langchain_core.messages import SystemMessage, HumanMessage, AIMessage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:20:16.552359Z","iopub.execute_input":"2025-07-08T17:20:16.552686Z","iopub.status.idle":"2025-07-08T17:20:16.557203Z","shell.execute_reply.started":"2025-07-08T17:20:16.552656Z","shell.execute_reply":"2025-07-08T17:20:16.556486Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"messages = [\n    SystemMessage(content= \"You are a helpful assistant\"),\n    HumanMessage(content=\"Tell me about langchain\")\n]\n\nresult = model.invoke(messages)\nmessages.append(\n    AIMessage(content = result.content.split('<|assistant|>\\n')[1])\n)\n\nprint(messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:22:37.741907Z","iopub.execute_input":"2025-07-08T17:22:37.742196Z","iopub.status.idle":"2025-07-08T17:22:39.071630Z","shell.execute_reply.started":"2025-07-08T17:22:37.742176Z","shell.execute_reply":"2025-07-08T17:22:39.070969Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about langchain', additional_kwargs={}, response_metadata={}), AIMessage(content=' Langchain is an AI-powered development platform that aims to simplify the process of building', additional_kwargs={}, response_metadata={})]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Chatbot using these message prompt","metadata":{}},{"cell_type":"code","source":"from pprint import pprint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:27:13.660438Z","iopub.execute_input":"2025-07-08T17:27:13.661207Z","iopub.status.idle":"2025-07-08T17:27:13.664716Z","shell.execute_reply.started":"2025-07-08T17:27:13.661181Z","shell.execute_reply":"2025-07-08T17:27:13.664094Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"chat_history = [\n    SystemMessage(content = \"You are a helpful AI assistant\")\n]\nwhile True:\n    user_input = input(\"You: \")\n    chat_history.append(\n        HumanMessage(content = user_input)\n    )\n    if user_input == \"exit\":\n        break\n    result = model.invoke(chat_history)\n    ai_reply = result.content.split('<|assistant|>\\n')[-1]\n    chat_history.append(\n        AIMessage(content = ai_reply)\n    )\n    print(\"AI: \", ai_reply)\n\npprint(chat_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:27:15.778868Z","iopub.execute_input":"2025-07-08T17:27:15.779162Z","iopub.status.idle":"2025-07-08T17:27:25.628068Z","shell.execute_reply.started":"2025-07-08T17:27:15.779144Z","shell.execute_reply":"2025-07-08T17:27:25.627235Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Hi\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI:   Hello! How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  how are you?\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI:   I'm doing well, thank you for asking! How about yourself? How may I help you\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n AIMessage(content=' Hello! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={}),\n AIMessage(content=\" I'm doing well, thank you for asking! How about yourself? How may I help you\", additional_kwargs={}, response_metadata={}),\n HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Above shows proper tracing of message - System, Human and AI","metadata":{}},{"cell_type":"markdown","source":"## ChatPromptTemplate\n\nThis is useful for dynamic prompting when we don't want to give the full control to user","metadata":{}},{"cell_type":"code","source":"!pip install langchain_core","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:29:37.306710Z","iopub.execute_input":"2025-07-08T17:29:37.307392Z","iopub.status.idle":"2025-07-08T17:29:40.650409Z","shell.execute_reply.started":"2025-07-08T17:29:37.307365Z","shell.execute_reply":"2025-07-08T17:29:40.649587Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.68)\nRequirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (0.4.4)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.4)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (3.10.16)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (2.32.3)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.4.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (2.4.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.3.1)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\n\nchat_template = ChatPromptTemplate([\n    SystemMessage(content = \"You are a helpful {domain} expert\"),\n    HumanMessage(content = \"Explain in 1 line, what is {topic}\")\n])\nprompt = chat_template.invoke({'domain': 'cricket', 'topic': 'runout'})\npprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:33:51.036251Z","iopub.execute_input":"2025-07-08T17:33:51.036794Z","iopub.status.idle":"2025-07-08T17:33:51.042335Z","shell.execute_reply.started":"2025-07-08T17:33:51.036764Z","shell.execute_reply":"2025-07-08T17:33:51.041749Z"}},"outputs":[{"name":"stdout","text":"ChatPromptValue(messages=[SystemMessage(content='You are a helpful {domain} expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in 1 line, what is {topic}', additional_kwargs={}, response_metadata={})])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"#### ChatPromptTemplate does not work like PromptTemplate and it does not accept classes. Instead it takes in string format","metadata":{}},{"cell_type":"code","source":"chat_template = ChatPromptTemplate([\n    (\"system\", \"You are a helpful {domain} expert\"),\n    (\"human\", \"Explain in 1 line, what is {topic}\")\n])\nprompt = chat_template.invoke({'domain': 'cricket', 'topic': 'runout'})\npprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:35:59.410066Z","iopub.execute_input":"2025-07-08T17:35:59.410702Z","iopub.status.idle":"2025-07-08T17:35:59.417390Z","shell.execute_reply.started":"2025-07-08T17:35:59.410676Z","shell.execute_reply":"2025-07-08T17:35:59.416683Z"}},"outputs":[{"name":"stdout","text":"ChatPromptValue(messages=[SystemMessage(content='You are a helpful cricket expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in 1 line, what is runout', additional_kwargs={}, response_metadata={})])\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"chat_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful {domain} expert\"),\n    (\"human\", \"Explain in 1 line, what is {topic}\")\n])\nprompt = chat_template.invoke({'domain': 'cricket', 'topic': 'runout'})\npprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:36:12.005301Z","iopub.execute_input":"2025-07-08T17:36:12.005570Z","iopub.status.idle":"2025-07-08T17:36:12.011652Z","shell.execute_reply.started":"2025-07-08T17:36:12.005551Z","shell.execute_reply":"2025-07-08T17:36:12.010886Z"}},"outputs":[{"name":"stdout","text":"ChatPromptValue(messages=[SystemMessage(content='You are a helpful cricket expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in 1 line, what is runout', additional_kwargs={}, response_metadata={})])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Message Placeholder - use to insert chat history or a list of messages at runtime","metadata":{}},{"cell_type":"code","source":"%%writefile chathistory.txt\nHumanMessage(content=\"I want to request a refund for my order #1234\")\nAIMessage(content = \"Your refund for order #1234 has been initiated. It will be processed in 3-5 business days\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:41:00.352524Z","iopub.execute_input":"2025-07-08T17:41:00.353155Z","iopub.status.idle":"2025-07-08T17:41:00.358341Z","shell.execute_reply.started":"2025-07-08T17:41:00.353118Z","shell.execute_reply":"2025-07-08T17:41:00.357578Z"}},"outputs":[{"name":"stdout","text":"Writing chathistory.txt\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from langchain_core.prompts import MessagesPlaceholder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:42:29.205313Z","iopub.execute_input":"2025-07-08T17:42:29.206208Z","iopub.status.idle":"2025-07-08T17:42:29.209452Z","shell.execute_reply.started":"2025-07-08T17:42:29.206182Z","shell.execute_reply":"2025-07-08T17:42:29.208871Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"chat_template = ChatPromptTemplate([\n    (\"system\", \"You are a helpful assistant\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"{query}\")\n])\n\nchat_history = []\nwith open(\"/kaggle/working/chathistory.txt\") as f:\n    chat_history.extend(f.readlines())\n\nprint(chat_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:45:02.672160Z","iopub.execute_input":"2025-07-08T17:45:02.672446Z","iopub.status.idle":"2025-07-08T17:45:02.678360Z","shell.execute_reply.started":"2025-07-08T17:45:02.672419Z","shell.execute_reply":"2025-07-08T17:45:02.677713Z"}},"outputs":[{"name":"stdout","text":"['HumanMessage(content=\"I want to request a refund for my order #1234\")\\n', 'AIMessage(content = \"Your refund for order #1234 has been initiated. It will be processed in 3-5 business days\")\\n']\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"chat_template.invoke({'chat_history': chat_history, \"query\": \"Where is my refund\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:45:37.681817Z","iopub.execute_input":"2025-07-08T17:45:37.682104Z","iopub.status.idle":"2025-07-08T17:45:37.688497Z","shell.execute_reply.started":"2025-07-08T17:45:37.682086Z","shell.execute_reply":"2025-07-08T17:45:37.687949Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='HumanMessage(content=\"I want to request a refund for my order #1234\")\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='AIMessage(content = \"Your refund for order #1234 has been initiated. It will be processed in 3-5 business days\")\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where is my refund', additional_kwargs={}, response_metadata={})])"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"chain = chat_template | model\nresponse = chain.invoke({'chat_history': chat_history, \"query\": \"Where is my refund\"})\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:47:43.947166Z","iopub.execute_input":"2025-07-08T17:47:43.947822Z","iopub.status.idle":"2025-07-08T17:47:45.521759Z","shell.execute_reply.started":"2025-07-08T17:47:43.947792Z","shell.execute_reply":"2025-07-08T17:47:45.521072Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"content='<|system|>\\nYou are a helpful assistant<|end|>\\n<|user|>\\nHumanMessage(content=\"I want to request a refund for my order #1234\")\\n<|end|>\\n<|user|>\\nAIMessage(content = \"Your refund for order #1234 has been initiated. It will be processed in 3-5 business days\")\\n<|end|>\\n<|user|>\\nWhere is my refund<|end|>\\n<|assistant|>\\n I\\'m sorry, but as an AI, I don\\'t have access to your personal' additional_kwargs={} response_metadata={} id='run--a3e38002-53c8-48bc-b822-8349c8cd91a4-0'\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}